{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[Chainer](http://chainer.org/) とはニューラルネットの実装を簡単にしたフレームワークです。\n",
    "\n",
    "* 今回は言語の分野でニューラルネットを適用してみました。\n",
    "\n",
    "![](./pictures/Chainer.jpg)\n",
    "\n",
    "* 今回は言語モデルを作成していただきます。\n",
    "\n",
    "\n",
    "言語モデルとはある単語が来たときに次の単語に何が来やすいかを予測するものです。\n",
    "\n",
    "言語モデルにはいくつか種類があるのでここでも紹介しておきます。\n",
    "\n",
    "* n-グラム言語モデル\n",
    " * 単語の数を単純に数え挙げて作成されるモデル。考え方としてはデータにおけるある単語の頻度に近い\n",
    "* ニューラル言語モデル\n",
    " * 単語の辞書ベクトルを潜在空間ベクトルに落とし込み、ニューラルネットで次の文字を学習させる手法\n",
    "\n",
    "* リカレントニューラル言語モデル\n",
    " * 基本的なアルゴリズムはニューラル言語モデルと同一だが過去に使用した単語を入力に加えることによって文脈を考慮した言語モデルの学習が可能となる。ニューラル言語モデルとは異なり、より古い情報も取得可能\n",
    "\n",
    "以下では、このChainerを利用しデータを準備するところから実際に言語モデルを構築し学習・評価を行うまでの手順を解説します。\n",
    "\n",
    "1. [各種ライブラリ導入](#各種ライブラリ導入) \n",
    "2. [初期設定](#初期設定) \n",
    "3. [データ入力](#データ入力)\n",
    "4. [リカレントニューラル言語モデル設定](#リカレントニューラル言語モデル設定) \n",
    "5. [学習を始める前の設定](#学習を始める前の設定)\n",
    "6. [パラメータ更新方法（確率的勾配法）](#パラメータ更新方法（確率的勾配法）)\n",
    "7. [言語の予測](#言語の予測)\n",
    "\n",
    "もしGPUを使用したい方は、以下にまとめてあるのでご参考ください。\n",
    "\n",
    "[Chainer を用いてリカレントニューラル言語モデル作成のサンプルコードを解説してみた](http://qiita.com/GushiSnow/private/b34da4962dd930d1487a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各種ライブラリ導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerの言語処理では多数のライブラリを導入します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from chainer import cuda, Variable, FunctionSet, optimizers\n",
    "import chainer.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`導入するライブラリの代表例は下記です。\n",
    "\n",
    "* `numpy`: 行列計算などの複雑な計算を行なうライブラリ\n",
    "* `chainer`: Chainerの導入\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初期設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記を設定しています。\n",
    "* 学習回数：n_epochs\n",
    "* ニューラルネットのユニット数：n_units\n",
    "* 確率的勾配法に使用するデータの数：batchsize\n",
    "* 学習に使用する文字列の長さ：bprop_len\n",
    "* 勾配法で使用する敷居値：grad_clip\n",
    "* 学習データの格納場所：data_dir\n",
    "* モデルの出力場所：checkpoint_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-------------Explain7 in the Qiita-------------\n",
    "n_epochs    = 10\n",
    "n_units     = 128\n",
    "batchsize   = 50\n",
    "bprop_len   = 50\n",
    "grad_clip   = 5\n",
    "data_dir = \"data_hands_on\"\n",
    "checkpoint_dir = \"cv\"\n",
    "#-------------Explain7 in the Qiita-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ入力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習用にダウンロードしたファイルをプログラムに読ませる処理を関数化しています\n",
    "文字列の場合は通常のデータと異なり、数字ベクトル化する必要があります。\n",
    "\n",
    "* 学習データをバイナリ形式で読み込んでいます。\n",
    "* 文字データを確保するための行列を定義しています。\n",
    "* データは単語をキー、語彙数の連番idを値とした辞書データにして行列データセットに登録しています。\n",
    "\n",
    "学習データ、単語の長さ、語彙数を取得しています。\n",
    "上記をそれぞれ行列データとして保持しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_hands_on/linux_source.c\n",
      "corpus length: 3490080\n",
      "vocab size: 80\n"
     ]
    }
   ],
   "source": [
    "# input data\n",
    "#-------------Explain1 in the Qiita-------------\n",
    "def load_data():\n",
    "    vocab = {}\n",
    "    print ('%s/linux_source.c'% data_dir)\n",
    "    words = open('%s/linux_source.c' % data_dir, 'rb').read()\n",
    "    words = list(words)\n",
    "    dataset = np.ndarray((len(words),), dtype=np.int32)\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "        dataset[i] = vocab[word]\n",
    "    print('corpus length:', len(words))\n",
    "    print('vocab size:', len(vocab))\n",
    "    return dataset, words, vocab\n",
    "#-------------Explain1 in the Qiita-------------\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "    \n",
    "train_data, words, vocab = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リカレントニューラル言語モデル設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNLM(リカレントニューラル言語モデルの設定を行っています）\n",
    "この部分で自由にモデルを変えることが出来ます。\n",
    "この部分でリカレントニューラル言語モデル独特の特徴を把握してもらうことが目的です。\n",
    "\n",
    "* EmbedIDで行列変換を行い、疎なベクトルを密なベクトルに変換しています。辞書データを、入力ユニット数分のデータに変換する処理（潜在ベクトル空間への変換）を行っています。\n",
    "* 出力が4倍の理由は入力層、入力制限層、出力制限層、忘却層をLSTMでは入力に使用するためです。LSTMの魔法の工夫について知りたい方は下記をご覧下さい。\n",
    "http://www.slideshare.net/nishio/long-shortterm-memory\n",
    "* `h1_in   = self.l1_x(F.dropout(h0, ratio=dropout_ratio, train=train)) + self.l1_h(state['h1'])`は隠れ層に前回保持した隠れ層の状態を入力することによってLSTMを実現しています。\n",
    "* `F.dropout`は過去の情報を保持しながらどれだけのdropoutでユニットを削るかを表しています。これにより過学習するのを抑えています。\n",
    " Drop outについては下記をご覧下さい。\n",
    "\n",
    " http://olanleed.hatenablog.com/entry/2013/12/03/010945\n",
    "\n",
    "* `c1, h1  = F.lstm(state['c1'], h1_in)`はlstmと呼ばれる魔法の工夫によってリカレントニューラルネットがメモリ破綻を起こさずにいい感じで学習するための工夫です。詳しく知りたい人は下記をご覧下さい。\n",
    "* `return state, F.softmax_cross_entropy(y, t)`は予測した文字と実際の文字を比較して損失関数を更新している所になります。ソフトマックス関数を使用している理由は出力層の一つ前の層の全入力を考慮して出力を決定できるので一般的に出力層の計算にはソフトマックス関数が使用されます。\n",
    "* 予測を行なうメソッドも実装しており、入力されたデータ、状態を元に次の文字列と状態を返すような関数になっています。\n",
    "* モデルの初期化を行なう関数もここで定義しています。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(FunctionSet):\n",
    "\n",
    "#-------------Explain2 in the Qiita-------------\n",
    "    def __init__(self, n_vocab, n_units):\n",
    "        super(CharRNN, self).__init__(\n",
    "            embed = F.EmbedID(n_vocab, n_units),\n",
    "            l1_x = F.Linear(n_units, 4*n_units),\n",
    "            l1_h = F.Linear(n_units, 4*n_units),\n",
    "            l2_h = F.Linear(n_units, 4*n_units),\n",
    "            l2_x = F.Linear(n_units, 4*n_units),\n",
    "            l3   = F.Linear(n_units, n_vocab),\n",
    "        )\n",
    "        for param in self.parameters:\n",
    "            param[:] = np.random.uniform(-0.08, 0.08, param.shape)\n",
    "\n",
    "    def forward_one_step(self, x_data, y_data, state, train=True, dropout_ratio=0.5):\n",
    "        x = Variable(x_data, volatile=not train)\n",
    "        t = Variable(y_data, volatile=not train)\n",
    "\n",
    "        h0      = self.embed(x)\n",
    "        h1_in   = self.l1_x(F.dropout(h0, ratio=dropout_ratio, train=train)) + self.l1_h(state['h1'])\n",
    "        c1, h1  = F.lstm(state['c1'], h1_in)\n",
    "        h2_in   = self.l2_x(F.dropout(h1, ratio=dropout_ratio, train=train)) + self.l2_h(state['h2'])\n",
    "        c2, h2  = F.lstm(state['c2'], h2_in)\n",
    "        y       = self.l3(F.dropout(h2, ratio=dropout_ratio, train=train))\n",
    "        state   = {'c1': c1, 'h1': h1, 'c2': c2, 'h2': h2}\n",
    "\n",
    "        return state, F.softmax_cross_entropy(y, t)\n",
    "#-------------Explain2 in the Qiita-------------\n",
    "\n",
    "    def predict(self, x_data, state):\n",
    "        x = Variable(x_data, volatile=True)\n",
    "\n",
    "        h0      = self.embed(x)\n",
    "        h1_in   = self.l1_x(h0) + self.l1_h(state['h1'])\n",
    "        c1, h1  = F.lstm(state['c1'], h1_in)\n",
    "        h2_in   = self.l2_x(h1) + self.l2_h(state['h2'])\n",
    "        c2, h2  = F.lstm(state['c2'], h2_in)\n",
    "        y       = self.l3(h2)\n",
    "        state   = {'c1': c1, 'h1': h1, 'c2': c2, 'h2': h2}\n",
    "\n",
    "        return state, F.softmax(y)\n",
    "\n",
    "def make_initial_state(n_units, batchsize=50, train=True):\n",
    "    return {name: Variable(np.zeros((batchsize, n_units), dtype=np.float32),\n",
    "            volatile=not train)\n",
    "            for name in ('c1', 'h1', 'c2', 'h2')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNLM(リカレントニューラル言語モデルの設定を行っています）\n",
    "\n",
    "* 作成したリカレントニューラル言語モデルを導入しています。\n",
    "* 最適化の手法はRMSpropを使用\n",
    "http://qiita.com/skitaoka/items/e6afbe238cd69c899b2a\n",
    "* 初期のパラメータを-0.1〜0.1の間で与えています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare RNNLM model\n",
    "model = CharRNN(len(vocab), n_units)\n",
    "\n",
    "optimizer = optimizers.RMSprop(lr=2e-3, alpha=0.95, eps=1e-8)\n",
    "optimizer.setup(model.collect_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習を始める前の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 学習データのサイズを取得 \n",
    "* ジャンプの幅を設定（順次学習しない）\n",
    "* パープレキシティを0で初期化 \n",
    "* 最初の時間情報を取得 \n",
    "* 初期状態を現在の状態に付与 \n",
    "* 状態の初期化 \n",
    "* 損失を0で初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whole_len    = train_data.shape[0]\n",
    "jump         = whole_len / batchsize\n",
    "epoch        = 0\n",
    "start_at     = time.time()\n",
    "cur_at       = start_at\n",
    "state        = make_initial_state(n_units, batchsize=batchsize)\n",
    "accum_loss   = Variable(np.zeros((), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータ更新方法（確率的勾配法）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 確率的勾配法を用いて学習している。\n",
    "* `x_batch = np.array([train_data[(jump * j + i) % whole_len] for j in range(batchsize)])`はデータの一部を連続的に使用せず、飛ばしながら使用することでデータの偏りを防ぐように学習していきます。\n",
    "* `state, loss_i = model.forward_one_step(x_batch, y_batch, state, dropout_ratio=0.5)`は損失と状態を計算しています。ここで過学習を防ぐdropアウトの率も設定可能です。\n",
    "* `if (i + 1) % bprop_len == 0`はどれだけ過去の文字を保持するかを表しています。bprop_lenが大きければ大きいほど過去の文字を保持できますが、メモリ破綻を起こす可能性があるのでタスクによって適切な数値に設定する必要があります。\n",
    "* bprop_lenの詳細について[truncate](http://kiyukuta.github.io/2013/12/09/mlac2013_day9_recurrent_neural_network_language_model.html#recurrent-neural-network)\n",
    "* `optimizer.clip_grads(grad_clip)`は正則化をかけており、過学習を防いでいます。\n",
    "* `optimizer.lr *= 0.97`は学習を緩やかにしていく部分です。最初は学習は大きく更新したいですが、だんだんと最適な解に近づいているため緩やかに近づける調整のための工夫です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0/69801.6, train_loss = 4.375024108886719, time = 3.44\n",
      "2.0/69801.6, train_loss = 4.294446105957031, time = 0.71\n",
      "3.0/69801.6, train_loss = 3.878165588378906, time = 0.67\n",
      "4.0/69801.6, train_loss = 3.92676025390625, time = 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohgushimasaya/.pyenv/versions/3.4.1/lib/python3.4/site-packages/ipykernel/__main__.py:4: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/ohgushimasaya/.pyenv/versions/3.4.1/lib/python3.4/site-packages/ipykernel/__main__.py:6: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-95e0c2c8da26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0maccum_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0maccum_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munchain_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# truncate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0maccum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ohgushimasaya/.pyenv/versions/3.4.1/lib/python3.4/site-packages/chainer/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, retain_grad)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data_type_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musing_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0mgxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ohgushimasaya/.pyenv/versions/3.4.1/lib/python3.4/site-packages/chainer/function.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, inputs, grad_outputs)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ohgushimasaya/.pyenv/versions/3.4.1/lib/python3.4/site-packages/chainer/functions/embed_id.py\u001b[0m in \u001b[0;36mbackward_cpu\u001b[0;34m(self, x, gy)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(int(jump * n_epochs)):\n",
    "    #-------------Explain4 in the Qiita-------------\n",
    "    x_batch = np.array([train_data[(jump * j + i) % whole_len]\n",
    "                        for j in range(batchsize)])\n",
    "    y_batch = np.array([train_data[(jump * j + i + 1) % whole_len]\n",
    "                        for j in range(batchsize)])\n",
    "\n",
    "    state, loss_i = model.forward_one_step(x_batch, y_batch, state, dropout_ratio=0.5)\n",
    "    accum_loss   += loss_i\n",
    "\n",
    "    if (i + 1) % bprop_len == 0:  # Run truncated BPTT\n",
    "        now = time.time()\n",
    "        print('{}/{}, train_loss = {}, time = {:.2f}'.format((i+1)/bprop_len, jump, accum_loss.data / bprop_len, now-cur_at))\n",
    "        cur_at = now\n",
    "\n",
    "        optimizer.zero_grads()\n",
    "        accum_loss.backward()\n",
    "        accum_loss.unchain_backward()  # truncate\n",
    "        accum_loss = Variable(np.zeros((), dtype=np.float32))\n",
    "\n",
    "        optimizer.clip_grads(grad_clip)\n",
    "        optimizer.update()\n",
    "\n",
    "    if (i + 1) % 10000 == 0:    \n",
    "        fn = ('%s/charrnn_epoch_%.2f.chainermodel' % (checkpoint_dir, float(i)/jump))\n",
    "        pickle.dump(copy.deepcopy(model).to_cpu(), open(fn, 'wb'))\n",
    "\n",
    "    if (i + 1) % jump == 0:\n",
    "        epoch += 1\n",
    "\n",
    "        if epoch >= 10:\n",
    "            optimizer.lr *= 0.97\n",
    "            print('decayed learning rate by a factor {} to {}'.format(0.97, optimizer.lr))\n",
    "    #-------------Explain4 in the Qiita-------------\n",
    "\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語の予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習時はバイナリでデータを取得していましたが、予測は通常のファイル形式で取得しています。\n",
    "* `ivocab = {v:k for k, v in vocab.items()}`でインデックスから単語を取得できるように並べ替えをしています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "# load vocabulary\n",
    "vocab = {}\n",
    "#-------------Explain5 in the Qiita-------------\n",
    "def load_predict_data(filename):\n",
    "    global vocab, n_vocab\n",
    "    #words = open(filename).read().replace('\\n', '<eos>').strip().split()\n",
    "    words = open(filename).read().strip().split()\n",
    "    dataset = np.ndarray((len(words),), dtype=np.int32)\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "        dataset[i] = vocab[word]\n",
    "    return dataset\n",
    "\n",
    "train_data = load_predict_data('data_hands_on/linux_source.c')\n",
    "\n",
    "ivocab = {}\n",
    "ivocab = {v:k for k, v in vocab.items()}\n",
    "#-------------Explain5 in the Qiita-------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "予測では作成されたモデル変更と文字列予測を行ないます。\n",
    "\n",
    "* モデルを変更する。\n",
    "* 文字列を予測する。\n",
    "\n",
    "予測するモデルの変更はここではiPython notebook内の下記のコードを変更します。\n",
    "作成されたモデルはcvフォルダの中にあるので\n",
    "あまり数は出来ていませんが、確認して見て下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "#-------------Explain6 in the Qiita-------------\n",
    "model = pickle.load(open(\"cv/charrnn_epoch_0.14.chainermodel\", 'rb'))\n",
    "#-------------Explain6 in the Qiita-------------\n",
    "n_units = model.embed.W.shape[1]\n",
    "\n",
    "# initialize generator\n",
    "state = make_initial_state(n_units, batchsize=1, train=False)\n",
    "\n",
    "prev_char = np.array([0], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `state, prob = model.predict(prev_char, state)`で予測された確率と状態を取得しています。次の予測にも使用するため状態も取得しています。\n",
    "* `index = np.argmax(cuda.to_cpu(prob.data))`は`cuda.to_cpu(prob.data)`部分で各単語の重み確率を取得できるため、その中で一番確率が高いものが予測された文字なのでその文字のインデックスを返すようにしています。\n",
    "* `sys.stdout.write(ivocab[index] + \" \")`で予測した文字を出力するための準備です。\n",
    "* `prev_char = np.array([index], dtype=np.int32)`は次の予測に使用するために過去の文字を保持するのに使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& & void { \"Memory { inb_p); 0xFFFFFFFF slist->bytes); (bit { { { { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { { { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) 0xFFFFFFFF { mutex_unlock(&s->sock->mutex); { (in_8(&ch->ch_flags) slist->bytes); (bit { { { { { { { { { { { { { { { "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    #-------------Explain7 in the Qiita-------------\n",
    "    state, prob = model.predict(prev_char, state)\n",
    "\n",
    "    index = np.argmax(cuda.to_cpu(prob.data))\n",
    "    sys.stdout.write(ivocab[index] + \" \")\n",
    "\n",
    "    prev_char = np.array([index], dtype=np.int32)\n",
    "    #-------------Explain7 in the Qiita-------------\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
