{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[Chainer](http://chainer.org/) とはニューラルネットの実装を簡単にしたフレームワークです。\n",
    "\n",
    "* 今回は言語の分野でニューラルネットを適用してみました。\n",
    "\n",
    "![](./pictures/Chainer.jpg)\n",
    "\n",
    "* 今回は言語モデルを作成していただきます。\n",
    "\n",
    "\n",
    "言語モデルとはある単語が来たときに次の単語に何が来やすいかを予測するものです。\n",
    "\n",
    "言語モデルにはいくつか種類があるのでここでも紹介しておきます。\n",
    "\n",
    "* n-グラム言語モデル\n",
    " * 単語の数を単純に数え挙げて作成されるモデル。考え方としてはデータにおけるある単語の頻度に近い\n",
    "* ニューラル言語モデル\n",
    " * 単語の辞書ベクトルを潜在空間ベクトルに落とし込み、ニューラルネットで次の文字を学習させる手法\n",
    "\n",
    "* リカレントニューラル言語モデル\n",
    " * 基本的なアルゴリズムはニューラル言語モデルと同一だが過去に使用した単語を入力に加えることによって文脈を考慮した言語モデルの学習が可能となる。ニューラル言語モデルとは異なり、より古い情報も取得可能\n",
    "\n",
    "以下では、このChainerを利用しデータを準備するところから実際に言語モデルを構築し学習・評価を行うまでの手順を解説します。\n",
    "\n",
    "1. [各種ライブラリ導入](#各種ライブラリ導入) \n",
    "2. [初期設定](#初期設定) \n",
    "3. [データ入力](#データ入力)\n",
    "4. [リカレントニューラル言語モデル設定](#リカレントニューラル言語モデル設定(ハンズオン）) \n",
    "5. [学習を始める前の設定](#学習を始める前の設定)\n",
    "6. [パラメータ更新方法](#パラメータ更新方法（ミニバッチ学習）)\n",
    "7. [言語の予測](#言語の予測)\n",
    "\n",
    "もしGPUを使用したい方は、以下にまとめてあるのでご参考ください。\n",
    "\n",
    "[Chainer を用いてリカレントニューラル言語モデル作成のサンプルコードを解説してみた](http://qiita.com/GushiSnow/private/b34da4962dd930d1487a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各種ライブラリ導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerの言語処理では多数のライブラリを導入します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from chainer import cuda, Variable, FunctionSet, optimizers\n",
    "import chainer.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`導入するライブラリの代表例は下記です。\n",
    "\n",
    "* `numpy`: 行列計算などの複雑な計算を行なうライブラリ\n",
    "* `chainer`: Chainerの導入\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初期設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記を設定しています。\n",
    "* 学習回数：n_epochs\n",
    "* ニューラルネットのユニット数：n_units\n",
    "* 確率的勾配法に使用するデータの数：batchsize\n",
    "* 学習に使用する文字列の長さ：bprop_len\n",
    "* 勾配法で使用する敷居値：grad_clip\n",
    "* 学習データの格納場所：data_dir\n",
    "* モデルの出力場所：checkpoint_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-------------Explain7 in the Qiita-------------\n",
    "n_epochs    = 5\n",
    "n_units     = 50\n",
    "batchsize   = 100\n",
    "bprop_len   = 10\n",
    "grad_clip   = 1\n",
    "data_dir = \"data_hands_on\"\n",
    "checkpoint_dir = \"cv\"\n",
    "#-------------Explain7 in the Qiita-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ入力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習用にダウンロードしたファイルをプログラムに読ませる処理を関数化しています\n",
    "文字列の場合は通常のデータと異なり、数字ベクトル化する必要があります。\n",
    "\n",
    "* 学習データをバイナリ形式で読み込んでいます。\n",
    "* 文字データを確保するための行列を定義しています。\n",
    "* データは単語をキー、語彙数の連番idを値とした辞書データにして行列データセットに登録しています。\n",
    "\n",
    "学習データ、単語の長さ、語彙数を取得しています。\n",
    "上記をそれぞれ行列データとして保持しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input data\n",
    "#-------------Explain1 in the Qiita-------------\n",
    "def source_to_words(source):\n",
    "    line = source.replace(\"¥n\", \" \").replace(\"¥t\", \" \")\n",
    "    for spacer in [\"(\", \")\", \"{\", \"}\", \"[\", \"]\", \",\"]:\n",
    "        line = line.replace(spacer, \" \" + spacer + \" \")\n",
    "    \n",
    "    words = [w.strip() for w in line.split()]\n",
    "    return words\n",
    "\n",
    "def load_data():\n",
    "    vocab = {}\n",
    "    print ('%s/angular.js'% data_dir)\n",
    "    words = open('%s/angular_full_remake.js' % data_dir, 'r').read()\n",
    "    words = source_to_words(words)\n",
    "    dataset = np.ndarray((len(words),), dtype=np.int32)\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "        dataset[i] = vocab[word]\n",
    "    print('corpus length:', len(words))\n",
    "    print('vocab size:', len(vocab))\n",
    "    return dataset, words, vocab\n",
    "#-------------Explain1 in the Qiita-------------\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "    \n",
    "train_data, words, vocab = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リカレントニューラル言語モデル設定(ハンズオン）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNLM(リカレントニューラル言語モデルの設定を行っています）\n",
    "この部分で自由にモデルを変えることが出来ます。\n",
    "この部分でリカレントニューラル言語モデル独特の特徴を把握してもらうことが目的です。\n",
    "\n",
    "* EmbedIDで行列変換を行い、疎なベクトルを密なベクトルに変換しています。辞書データを、入力ユニット数分のデータに変換する処理（潜在ベクトル空間への変換）を行っています。\n",
    "* 出力が4倍の理由は入力層、入力制限層、出力制限層、忘却層をLSTMでは入力に使用するためです。LSTMの魔法の工夫について知りたい方は下記をご覧下さい。\n",
    "http://www.slideshare.net/nishio/long-shortterm-memory\n",
    "* `h1_in   = self.l1_x(F.dropout(h0, ratio=dropout_ratio, train=train)) + self.l1_h(state['h1'])`は隠れ層に前回保持した隠れ層の状態を入力することによってLSTMを実現しています。\n",
    "* `F.dropout`は過去の情報を保持しながらどれだけのdropoutでユニットを削るかを表しています。これにより過学習するのを抑えています。\n",
    " Drop outについては下記をご覧下さい。\n",
    "\n",
    " http://olanleed.hatenablog.com/entry/2013/12/03/010945\n",
    "\n",
    "* `c1, h1  = F.lstm(state['c1'], h1_in)`はlstmと呼ばれる魔法の工夫によってリカレントニューラルネットがメモリ破綻を起こさずにいい感じで学習するための工夫です。詳しく知りたい人は下記をご覧下さい。\n",
    "* `return state, F.softmax_cross_entropy(y, t)`は予測した文字と実際の文字を比較して損失関数を更新している所になります。ソフトマックス関数を使用している理由は出力層の一つ前の層の全入力を考慮して出力を決定できるので一般的に出力層の計算にはソフトマックス関数が使用されます。\n",
    "* 予測を行なうメソッドも実装しており、入力されたデータ、状態を元に次の文字列と状態を返すような関数になっています。\n",
    "* モデルの初期化を行なう関数もここで定義しています。\n",
    "\n",
    "下記をコーディングして下さい！！！！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------Explain2 in the Qiita-------------\n",
    "class CharRNN(FunctionSet):\n",
    "\n",
    "    \"\"\"\n",
    "    ニューラルネットワークを定義している部分です。\n",
    "    上から順に入力された辞書ベクトル空間を隠れ層のユニット数に変換し、次に隠れ層の入\n",
    "    力と隠れ層を設定しています。\n",
    "    同様の処理を2層にも行い、出力層では語彙数に修正して出力しています。\n",
    "    なお最初に設定するパラメータは-0.08から0.08の間でランダムに設定しています。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_vocab, n_units):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    順伝搬の記述です。\n",
    "    順伝搬の入力をVariableで定義し、入力と答えを渡しています。\n",
    "    入力層を先ほど定義したembedを用います。\n",
    "    隠れ層の入力には、先ほど定義したl1_xを用いて、引数にdropout、隠れ層の状態を渡して\n",
    "    います。\n",
    "    lstmに隠れ層第1層の状態とh1_inを渡します。\n",
    "    2層目も同様に記述し、出力層は状態を渡さずに定義します。\n",
    "    次回以降の入力に使用するため各状態は保持しています。\n",
    "    出力されたラベルと答えのラベル比較し、損失を返すのと状態を返しています。\n",
    "    \"\"\"\n",
    "\n",
    "    def forward_one_step(self, x_data, y_data, state, train=True, dropout_ratio=0.5):\n",
    "\n",
    "    \"\"\"\n",
    "    dropoutの記述を外して予測用のメソッドとして記述しています。\n",
    "    dropoutにはtrainという引数が存在し、trainの引数をfalseにしておくと動作しない\n",
    "    ので、予測の時は渡す引数を変えて学習と予測を変えても良いですが、今回は明示的に分る\n",
    "    ように分けて記述しました。\n",
    "    \"\"\"\n",
    "\n",
    "    def predict(self, x_data, state):\n",
    "    \n",
    "\"\"\"\n",
    "状態の初期化です。\n",
    "\"\"\"\n",
    "\n",
    "def make_initial_state(n_units, batchsize=100, train=True):\n",
    "#-------------Explain2 in the Qiita-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNLM(リカレントニューラル言語モデルの設定を行っています）\n",
    "\n",
    "* 作成したリカレントニューラル言語モデルを導入しています。\n",
    "* 最適化の手法はRMSpropを使用\n",
    "http://qiita.com/skitaoka/items/e6afbe238cd69c899b2a\n",
    "* 初期のパラメータを-0.1〜0.1の間で与えています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare RNNLM model\n",
    "model = CharRNN(len(vocab), n_units)\n",
    "\n",
    "optimizer = optimizers.RMSprop(lr=2e-3, alpha=0.95, eps=1e-8)\n",
    "optimizer.setup(model.collect_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習を始める前の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 学習データのサイズを取得 \n",
    "* ジャンプの幅を設定（順次学習しない）\n",
    "* パープレキシティを0で初期化 \n",
    "* 最初の時間情報を取得 \n",
    "* 初期状態を現在の状態に付与 \n",
    "* 状態の初期化 \n",
    "* 損失を0で初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whole_len    = train_data.shape[0]\n",
    "jump         = whole_len / batchsize\n",
    "epoch        = 0\n",
    "start_at     = time.time()\n",
    "cur_at       = start_at\n",
    "state        = make_initial_state(n_units, batchsize=batchsize)\n",
    "accum_loss   = Variable(np.zeros((), dtype=np.float32))\n",
    "cur_log_perp = np.zeros(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータ更新方法（ミニバッチ学習）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ミニバッチを用いて学習している。\n",
    "* `x_batch = np.array([train_data[(jump * j + i) % whole_len] for j in range(batchsize)])`はデータの一部を連続的に使用せず、飛ばしながら使用することでデータの偏りを防ぐように学習していきます。\n",
    "* `state, loss_i = model.forward_one_step(x_batch, y_batch, state, dropout_ratio=0.5)`は損失と状態を計算しています。ここで過学習を防ぐdropアウトの率も設定可能です。\n",
    "* `if (i + 1) % bprop_len == 0`はどれだけ過去の文字を保持するかを表しています。bprop_lenが大きければ大きいほど過去の文字を保持できますが、メモリ破綻を起こす可能性があるのでタスクによって適切な数値に設定する必要があります。\n",
    "* bprop_lenの詳細について[truncate](http://kiyukuta.github.io/2013/12/09/mlac2013_day9_recurrent_neural_network_language_model.html#recurrent-neural-network)\n",
    "* `optimizer.clip_grads(grad_clip)`は正則化をかけており、過学習を防いでいます。\n",
    "* `optimizer.lr *= 0.97`は学習を緩やかにしていく部分です。最初は学習は大きく更新したいですが、だんだんと最適な解に近づいているため緩やかに近づける調整のための工夫です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(int(jump * n_epochs)):\n",
    "    #-------------Explain4 in the Qiita-------------\n",
    "    x_batch = np.array([train_data[(jump * j + i) % whole_len]\n",
    "                        for j in range(batchsize)])\n",
    "    y_batch = np.array([train_data[(jump * j + i + 1) % whole_len]\n",
    "                        for j in range(batchsize)])\n",
    "\n",
    "    state, loss_i = model.forward_one_step(x_batch, y_batch, state, dropout_ratio=0.7)\n",
    "    accum_loss   += loss_i\n",
    "    cur_log_perp += loss_i.data.reshape(())\n",
    "\n",
    "    if (i + 1) % bprop_len == 0:  # Run truncated BPTT\n",
    "        now = time.time()\n",
    "\n",
    "        print('{}/{}, train_loss = {}, time = {:.2f}'.format((i+1)/bprop_len, jump, accum_loss.data / bprop_len, now-cur_at))\n",
    "        cur_at = now\n",
    "\n",
    "        optimizer.zero_grads()\n",
    "        accum_loss.backward()\n",
    "        accum_loss.unchain_backward()  # truncate\n",
    "        accum_loss = Variable(np.zeros((), dtype=np.float32))\n",
    "\n",
    "        optimizer.clip_grads(grad_clip)\n",
    "        optimizer.update()\n",
    "        \n",
    "    if (i + 1) % 10000 == 0:\n",
    "        perp = math.exp(cuda.to_cpu(cur_log_perp) / 10000)\n",
    "        print('iter {} training perplexity: {:.2f} '.format(i + 1, perp))\n",
    "        fn = ('%s/charrnn_epoch_%.2f.chainermodel' % (checkpoint_dir, float(i)/jump))\n",
    "        pickle.dump(copy.deepcopy(model).to_cpu(), open(fn, 'wb'))\n",
    "\n",
    "    if (i + 1) % jump == 0:\n",
    "        epoch += 1\n",
    "\n",
    "        if epoch >= 10:\n",
    "            optimizer.lr *= 0.97\n",
    "            print('decayed learning rate by a factor {} to {}'.format(0.97, optimizer.lr))\n",
    "    #-------------Explain4 in the Qiita-------------\n",
    "\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語の予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "予測では作成されたモデル変更と文字列予測を行ないます。\n",
    "\n",
    "* モデルを変更する。\n",
    "* 文字列を予測する。\n",
    "\n",
    "予測するモデルの変更はここではiPython notebook内の下記のコードを変更します。\n",
    "作成されたモデルはcvフォルダの中にあるので\n",
    "あまり数は出来ていませんが、確認して見て下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "#-------------Explain6 in the Qiita-------------\n",
    "model = pickle.load(open(\"cv/charrnn_epoch_4.51.chainermodel\", 'rb'))\n",
    "#-------------Explain6 in the Qiita-------------\n",
    "n_units = model.embed.W.shape[1]\n",
    "\n",
    "# initialize generator\n",
    "state = make_initial_state(n_units, batchsize=1, train=False)\n",
    "\n",
    "# show vocababury\n",
    "ivocab = {}\n",
    "ivocab = {v:k for k, v in vocab.items()}\n",
    "\n",
    "prev_char = np.array([0], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `state, prob = model.predict(prev_char, state)`で予測された確率と状態を取得しています。次の予測にも使用するため状態も取得しています。\n",
    "* `index = np.argmax(cuda.to_cpu(prob.data))`は`cuda.to_cpu(prob.data)`部分で各単語の重み確率を取得できるため、その中で一番確率が高いものが予測された文字なのでその文字のインデックスを返すようにしています。\n",
    "* `sys.stdout.write(ivocab[index] + \" \")`で予測した文字を出力するための準備です。\n",
    "* `prev_char = np.array([index], dtype=np.int32)`は次の予測に使用するために過去の文字を保持するのに使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r\".*;.*\")\n",
    "\n",
    "for i in range(1000):\n",
    "    #-------------Explain7 in the Qiita-------------\n",
    "    state, prob = model.predict(prev_char, state)\n",
    "    \n",
    "    index = np.argmax(cuda.to_cpu(prob.data))\n",
    "\n",
    "    if ivocab[index] == \"{\" or ivocab[index] == \"}\" or pattern.match(ivocab[index]):\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        split_vocab = ivocab[index].split(\";\")\n",
    "        sys.stdout.write(split_vocab[0] + \";\")\n",
    "        sys.stdout.write(\"\\n\") \n",
    "        if len(split_vocab) >= 2:\n",
    "            sys.stdout.write(split_vocab[1])\n",
    "    else:\n",
    "        sys.stdout.write(ivocab[index] + \" \")\n",
    "        \n",
    "    prev_char = np.array([index], dtype=np.int32)\n",
    "    #-------------Explain7 in the Qiita-------------\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
